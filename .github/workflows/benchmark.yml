name: Benchmark

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      url:
        description: 'URL to download for benchmarking'
        required: true
        default: 'https://sin-speed.hetzner.com/1GB.bin'
      iterations:
        description: 'Number of iterations'
        required: true
        default: '5'
      warmups:
        description: 'Warmup rounds (not counted)'
        required: true
        default: '1'
      connections:
        description: 'Shared connection count for Surge and aria2'
        required: true
        default: '16'
      surge_exit_check_interval:
        description: 'Surge --exit-check-interval for benchmark runs (e.g. 100ms)'
        required: true
        default: '100ms'
      baseline:
        description: 'Compare against baseline (main branch)? [y/n]'
        required: true
        default: 'n'
      speedtest:
        description: 'Run speedtest-cli? [y/n]'
        required: true
        default: 'n'

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    name: Run Benchmark
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Current Code
        uses: actions/checkout@v4
        with:
          path: current

      - name: Checkout Main Branch (Baseline)
        uses: actions/checkout@v4
        with:
          ref: main
          path: baseline

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.24.4+auto"
          cache-dependency-path: current/go.sum

      - name: Update APT
        run: sudo apt-get update

      - name: Install Benchmark Tools (Aria2, Axel, Speedtest)
        run: sudo apt-get install -y aria2 axel speedtest-cli

      - name: Build Current
        run: |
          cd current
          go build -o ../surge-current .

      - name: Build Baseline
        run: |
          cd baseline
          go build -o ../surge-baseline .

      - name: Run Benchmark
        id: benchmark
        env:
          # Use 'github.event.inputs' for manual triggers, default for others
          BENCH_URL: ${{ github.event.inputs.url || 'https://sin-speed.hetzner.com/1GB.bin' }}
          # On push/PR, we usually want 1 iteration to save time, 5 for manual
          BENCH_ITERATIONS: ${{ github.event.inputs.iterations || '10' }}
          BENCH_WARMUPS: ${{ github.event.inputs.warmups || '1' }}
          BENCH_CONNECTIONS: ${{ github.event.inputs.connections || '16' }}
          BENCH_SURGE_EXIT_CHECK_INTERVAL: ${{ github.event.inputs.surge_exit_check_interval || '100ms' }}
          # Default to 'y' for PRs to see regressions, 'n' for simple pushes
          BASELINE_COMPARE: ${{ github.event.inputs.baseline || 'y' }}
          BENCH_SPEEDTEST: ${{ github.event.inputs.speedtest || 'n' }}

        run: |
          cd current
          
          # Build benchmark args (aligned with fair Surge vs aria2 comparison)
          ARGS=(
            "$BENCH_URL"
            "-n" "$BENCH_ITERATIONS"
            "--warmups" "$BENCH_WARMUPS"
            "--connections" "$BENCH_CONNECTIONS"
            "--surge-timing" "external"
            "--surge-exit-check-interval" "$BENCH_SURGE_EXIT_CHECK_INTERVAL"
            "--isolate-surge"
            "--aria2-no-conf"
            "--strict-size"
            "--surge"
            "--aria2"
          )

          # Add Baseline ONLY if requested
          if [[ "$BASELINE_COMPARE" == "y" ]]; then
            echo "Comparing against baseline..."
            ARGS+=("--surge-baseline" "../surge-baseline")
          fi

          # Optional speedtest
          if [[ "$BENCH_SPEEDTEST" == "y" ]]; then
            ARGS+=("--speedtest")
          fi

          # Run benchmark
          python3 benchmark.py "${ARGS[@]}" | tee benchmark_output.txt

          # Save output to GITHUB_ENV for the next step (multiline safe)
          echo "BENCHMARK_RESULTS<<EOF" >> $GITHUB_ENV
          cat benchmark_output.txt >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          # Write to Job Summary (Markdown)
          echo "## ðŸš€ Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "**Target:** $BENCH_URL" >> $GITHUB_STEP_SUMMARY
          echo "**Iterations:** $BENCH_ITERATIONS" >> $GITHUB_STEP_SUMMARY
          echo "**Warmups:** $BENCH_WARMUPS" >> $GITHUB_STEP_SUMMARY
          echo "**Connections:** $BENCH_CONNECTIONS" >> $GITHUB_STEP_SUMMARY
          echo "**Surge exit-check-interval:** $BENCH_SURGE_EXIT_CHECK_INTERVAL" >> $GITHUB_STEP_SUMMARY
          echo "**Tools:** surge, aria2" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat benchmark_output.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
