name: Benchmark

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      url:
        description: 'URL to download for benchmarking'
        required: true
        default: 'https://sin-speed.hetzner.com/1GB.bin'
      iterations:
        description: 'Number of iterations'
        required: true
        default: '5'
      warmups:
        description: 'Warmup rounds (not counted)'
        required: true
        default: '1'
      connections:
        description: 'Shared connection count for Surge and aria2'
        required: true
        default: '16'
      url_suite:
        description: 'Run a predefined multi-CDN URL suite? [y/n]'
        required: true
        default: 'y'
      surge_exit_check_interval:
        description: 'Surge --exit-check-interval for benchmark runs (e.g. 100ms)'
        required: true
        default: '100ms'
      baseline:
        description: 'Compare against baseline (main branch)? [y/n]'
        required: true
        default: 'n'
      speedtest:
        description: 'Run speedtest-cli? [y/n]'
        required: true
        default: 'n'

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    name: Run Benchmark
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Current Code
        uses: actions/checkout@v4
        with:
          path: current

      - name: Checkout Main Branch (Baseline)
        uses: actions/checkout@v4
        with:
          ref: main
          path: baseline

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.24.4+auto"
          cache-dependency-path: current/go.sum

      - name: Update APT
        run: sudo apt-get update

      - name: Install Benchmark Tools (Aria2, Axel, Speedtest)
        run: sudo apt-get install -y aria2 axel speedtest-cli

      - name: Build Current
        run: |
          cd current
          go build -o ../surge-current .

      - name: Build Baseline
        run: |
          cd baseline
          go build -o ../surge-baseline .

      - name: Run Benchmark
        id: benchmark
        env:
          # Use 'github.event.inputs' for manual triggers, default for others
          BENCH_URL: ${{ github.event.inputs.url || 'https://sin-speed.hetzner.com/1GB.bin' }}
          # On push/PR, we usually want 1 iteration to save time, 5 for manual
          BENCH_ITERATIONS: ${{ github.event.inputs.iterations || '10' }}
          BENCH_WARMUPS: ${{ github.event.inputs.warmups || '1' }}
          BENCH_CONNECTIONS: ${{ github.event.inputs.connections || '16' }}
          BENCH_URL_SUITE: ${{ github.event.inputs.url_suite || 'y' }}
          BENCH_SURGE_EXIT_CHECK_INTERVAL: ${{ github.event.inputs.surge_exit_check_interval || '100ms' }}
          # Default to 'n' unless explicitly requested (avoids mixed-binary flag incompatibility on PRs)
          BASELINE_COMPARE: ${{ github.event.inputs.baseline || 'n' }}
          BENCH_SPEEDTEST: ${{ github.event.inputs.speedtest || 'n' }}

        run: |
          cd current

          run_one() {
            local target_url="$1"
            local out_file="$2"

            # Build benchmark args (aligned with fair Surge vs aria2 comparison)
            ARGS=(
              "$target_url"
              "-n" "$BENCH_ITERATIONS"
              "--warmups" "$BENCH_WARMUPS"
              "--connections" "$BENCH_CONNECTIONS"
              "--surge-timing" "external"
              "--surge-exit-check-interval" "$BENCH_SURGE_EXIT_CHECK_INTERVAL"
              "--isolate-surge"
              "--aria2-no-conf"
              "--strict-size"
              "--surge-exec" "../surge-current"
              "--surge"
              "--aria2"
            )

            # Add Baseline ONLY if requested
            if [[ "$BASELINE_COMPARE" == "y" ]]; then
              echo "Comparing against baseline..."
              ARGS+=("--surge-baseline" "../surge-baseline")
            fi

            # Optional speedtest
            if [[ "$BENCH_SPEEDTEST" == "y" ]]; then
              ARGS+=("--speedtest")
            fi

            python3 benchmark.py "${ARGS[@]}" | tee "$out_file"
          }

          : > benchmark_output.txt

          if [[ "$BENCH_URL_SUITE" == "y" ]]; then
            # Multi-CDN suite for robustness (all use range-friendly large files)
            URLS=(
              "https://sin-speed.hetzner.com/1GB.bin"
              "https://proof.ovh.net/files/1Gb.dat"
              "https://mirror.leaseweb.com/speedtest/1GB.bin"
            )

            echo "Running multi-CDN benchmark suite (${#URLS[@]} URLs)..."
            for target_url in "${URLS[@]}"; do
              echo "" | tee -a benchmark_output.txt
              echo "================ URL: $target_url ================" | tee -a benchmark_output.txt
              run_one "$target_url" bench_single_output.txt
              cat bench_single_output.txt >> benchmark_output.txt
            done
          else
            run_one "$BENCH_URL" benchmark_output.txt
          fi

          # Save output to GITHUB_ENV for the next step (multiline safe)
          echo "BENCHMARK_RESULTS<<EOF" >> $GITHUB_ENV
          cat benchmark_output.txt >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          # Write to Job Summary (Markdown)
          echo "## ðŸš€ Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "**Target:** $BENCH_URL" >> $GITHUB_STEP_SUMMARY
          echo "**Iterations:** $BENCH_ITERATIONS" >> $GITHUB_STEP_SUMMARY
          echo "**Warmups:** $BENCH_WARMUPS" >> $GITHUB_STEP_SUMMARY
          echo "**Connections:** $BENCH_CONNECTIONS" >> $GITHUB_STEP_SUMMARY
          echo "**URL suite:** $BENCH_URL_SUITE" >> $GITHUB_STEP_SUMMARY
          echo "**Surge exit-check-interval:** $BENCH_SURGE_EXIT_CHECK_INTERVAL" >> $GITHUB_STEP_SUMMARY
          echo "**Tools:** surge, aria2" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat benchmark_output.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
